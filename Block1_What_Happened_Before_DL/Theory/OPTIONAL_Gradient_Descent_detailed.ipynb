{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In the previous studies we may have encountered the concept of optimum search, which uses techniques suitable for locating optima pretty independently of the properties of the problem space. None of the methods discussed in the general descriptions of optimum search rely too heavily on numeric properties of the solution space itself, thus are quite general in nature.\n",
    "\n",
    "None the less, we can argue, that if we do have some information regarding the properties of the problem space, we can use this to make our search for optima more efficient!\n",
    "\n",
    "## Example: Where to climb?\n",
    "\n",
    "Starting from the general situation of \"hillclimbing\", one of the weak spots for our approach is, that we are forced to try out neighboring solutions at random, that is, we have __no information about the \"goodness\" of neighbors__. The question is, at least in some problems, where certain numerical properties hold, can we maybe estimate - even when just locally - __in which direction we should take a step to decrease the error?__\n",
    "\n",
    "Turns out: we can do exactly that, if we can use eg. __gradient based methods__.\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In general hillclimbing like scenarios we do not really have too many requirements towards how the solution can be understood. For gradient based methods, though, we have to adapt a very specific setup, namely that of __function minimization__. \n",
    "\n",
    "One of the most notable consequences of this is, that we will concern ourselves with __continuous spaces__, that is, we will assume, that the underlying solution we are looking for is __scalar in nature__, so as a continuous, arbitrarily fine grained step can be taken during optimization, as well as the solution itself can potentially have arbitrary numeric floating point precision (under constraints of our broader hardware and software environment).\n",
    "\n",
    "We step away from __discrete or \"combinatorial\" optimization__, and enter __\"continuous\" optimization__. (In frames of this course we won't touch upon the third big field, \"stochastic\" optimization, where the solution itself has elements of randomness.)\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=19rTUagM5VsxO6BbkFQcb0cayYLIJPvwy\"><img src=\"https://drive.google.com/uc?export=view&id=1nw1e1KctcqmbanAjsMSU7XaR_-Ng4eGp\" width=45%></a>\n",
    "\n",
    "This necessarily leads us to a kind of relaxation of the notion of \"state\" and \"neighbors\", which we no longer consider to be \"atomic\", but much more as __current scalar parameters__ of some kind.\n",
    "\n",
    "\"Parameters of what?\" - One could ask, and rightfully so! In continuous optimization, it is dominantly assumed, that the solution of a problem has some kind of __\"functional form\"__, that is, it presents itself as a kind of function. In the above visual illustration, the continuous function was:\n",
    "\n",
    "$\\sqrt{2x}+1$\n",
    "\n",
    "If we are searching for the solution, we can consider this to be written in the form of:\n",
    "\n",
    "$\\sqrt{w_1x}+w_2$\n",
    "\n",
    "Where $w_1$ and $w_2$ are the unknown __scalar parameters__ we were searching for by successively modifying the $w$ parameters:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1ijaOwUgz4__luJSVNUP_Ku95tnYaY18H\"><img src=\"https://drive.google.com/uc?export=view&id=12bd4k0pvWR_UurxkEbTM2zUylMIbml-C\" width=\"45%\"></a>\n",
    "\n",
    "\n",
    "The big advantage of functional fitting is, that it allows naturally for __extrapolation__:\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/2160/1*aDllR5gUk-G4-Mjb4cbYPA.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1dP1z9rXiqzS_ycZAEBUMaF70FkbKF-cQ\" width=45%></a>\n",
    "\n",
    "Also observe, that this extrapolation behavior (hopefully, not in this case) can be problematic, because of __bad generalization__. Preventing such behavior is extensively discussed under the umbrella of [\"overfitting\"](https://en.wikipedia.org/wiki/Overfitting) in the statistics and machine learning literature, and is out of scope for the current course.\n",
    "\n",
    "Depending on the assumed functional form we optimize, we can talk about \"linear\", \"quadratic\", or just simply \"non-linear\" optimization. Though these represent radically different problems, none the less, gradient descent is applicable on all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not solve analytically?\n",
    "\n",
    "\n",
    "It is important to dwell on this assumption for a bit longer: It is the most typical case, as we have seen in this last animation above, that we only have a set of observations (the points above), and __we assume the functional form of the solution, for which we would like to estimate optimal parameters__, we \"fit the curve\", so to say, so we are looking for the __optimum of some error__ / difference function __with respect to the data__ at hand and __the assumed functional form__, and it's __parameters__.\n",
    "\n",
    "We can thus state, that we would like to find a parameter set that given the functional form we assume, produces the best local approximation to the observed datapoints. \n",
    "\n",
    "In the most \"lucky\" case, we had the right \"hypothesis\", so we assumed the functional form that is the description of the underlying process generating the data, thus when we will get to the optimal approximation, we describe the phenomena completely well, and thus we can rely on this in the future.\n",
    "\n",
    "So if we ourselves assume the functional form for fitting, why don't we use such functions, that have analytical solutions, and forego the whole search for optima altogether? Well, mainly because in real life situations, the data rarely comes from some very well behaved functions, and since future predictive value is our prime concern, __we have to assume complex functions__, thus, we don't have the luxury of closed form solutions. In fact, it is worth mentioning, that fifth or higher order equation systems do not typically have any closed form solution at all...\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=16AwVtO0epr7UN6tjWgvDBIkrVjglsdVo\"><img src=\"https://drive.google.com/uc?export=view&id=1MHT0RGIk0h1SG5ams71fs3B8yM_QSxPI\" width=45%></a>\n",
    "\n",
    "But it is also worth mentioning, that even in case of very complicated functional forms, __taking derivatives locally, with respect to certain points is often feasible__, so the work with derivatives (called in case of multiple variables __gradient vector__) is a very useful tool, that we can build upon! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual example of the usage of gradients\n",
    "\n",
    "Let us suppose, we continue using the function above:\n",
    "\n",
    "$f(x) = x^{5}+3.5x^{4}-2.5x^{3}-12.5x^{2}+1.5x+9$\n",
    "\n",
    "We decide, to do investigate the gangent line / derivative in the starting point $x=0.15$.\n",
    "\n",
    "Let us investigate visually, what this would mean!\n",
    "Please bear in mind, that tangent line calculation is __only for illustration__, it is not part of the GD procedure!\n",
    "\n",
    "### 1. getting the slope\n",
    "\n",
    "First derivative of it:\n",
    "$f'(x)=5x^{4}+14x^{3}-7.5x^{2}-25x+1.5$\n",
    "\n",
    "Calculating the derivative's value for $x=0.15$:\n",
    "\n",
    "$5×0.15^4+14×0.15^3−7.5×0.15^2−25×0.15+1.5 = −2.36896875$\n",
    "\n",
    "### 2. position of touching coordinates\n",
    "\n",
    "So we got the slope for the tangent line. Let us calculate it's touching position's coordinates!\n",
    "\n",
    "We know, that the two functions share a point at $x=0.15$, and we also know, that $f(0.15) = 8.937160313$, so we have a coordinate pair.\n",
    "\n",
    "### 3. calculating tangent function\n",
    "\n",
    "With the slope and the coordinates of the touching point, we can use the [point-slope formula](https://gato-docs.its.txstate.edu/slac/Subject/Math/Calculus/Findting-the-Equation-of-a-Tangent-Line/Finding) for calculating the tangent line's \"slope-intercept\" (\"usual\") formula.\n",
    "\n",
    "- \"The slope-intercept formula for a line is $y = mx + b$, where $m$ is the slope of the line and $b$ is the $y$-intercept.\n",
    "- The point-slope formula for a line is $y - y_1 = m (x - x_1)$. This formula uses a point on the line, denoted by $(x_1, y_1)$, and the slope of the line, denoted by $m$, to calculate the slope-intercept formula for the line.\"\n",
    "\n",
    "$y - y_1 = m (x - x_1)$\n",
    "\n",
    "$y - 8.937160313 = −2.36896875(x-0.15)$\n",
    "\n",
    "$y = −2.36896875(x-0.15) + 8.937160313$\n",
    "\n",
    "### 4. visualization\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1RqIErGPzNOgGE2JifE1qPkrGybjqRFJ2\"><img src=\"https://drive.google.com/uc?export=view&id=1SJBsAlYfAmrojaukirkB8jl3Y6zegl2c\" width=45%></a>\n",
    "\n",
    "From the above image, we can see, that if we \"follow along\" the gradient, that is, __the slope of the tangent line__ at the current position (which was $x=0.15$ in our case), and we __take a step in the direction of the NEGATED gradient__ ($-1*-2.36896875$), __we will step in the direction of a local minimum.__\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1ENd1cJjsTU-8blKL-194qJbMtADi8jnK\"><img src=\"https://drive.google.com/uc?export=view&id=18v04-zSnYmeu8HakN5mzPZrirRbtRRCh\" width=44%></a>\n",
    "\n",
    "Please observe, though, that the __direction__ of the step (\"increase $x$\") might be right, but if we would take the step literally, that is, we would make $x = 0.15 + 2.36896875$, we would take a too big of a __magnitude__ of step, that would lead us to a value, that is not even visible on this chart!\n",
    "\n",
    "Conceptually, with the __first derivative__, we created a __local, linear approximation__ of the function, that is only valid in a small $\\epsilon$ region, but starts to __deteriorate quickly__, thus the setting of appropriate __step size__ is a crucial question in GD. (For many methods trying to mitigate these problems in case of really complex problem surfaces, see separate notebook on GD variants.)  \n",
    "\n",
    "For a more robust approximation one can make use of the second derivatives (in fact, any [Taylor polynomial](https://en.wikipedia.org/wiki/Taylor_series)), which make our approximation better for a larger region.\n",
    "\n",
    "For a short intuition, see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T13:54:31.174502Z",
     "start_time": "2021-03-20T13:54:30.986775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/f0BxAtprWts?start=496\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f78882bf410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('f0BxAtprWts', start=496)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get back to the topic of second derivatives, and with them, second order methods in a later part of the lecture. \n",
    "\n",
    "### Extension to multiple dimensions\n",
    "\n",
    "Though up until now, we restricted our discussion to functions with on independent, and one dependent variable, but we have to realize, that nothing prevents us from calculating derivatives (in this case, really, properly called gradients) in a more complex functional plane.\n",
    "\n",
    "For example, if we take two independent, and one dependent variable, the __tangent becomes a plane__, and the __gradient a vector__ with components for the two independent variables, instead of only one scalar, as before. \n",
    "\n",
    "<a href=\"https://i.stack.imgur.com/nntba.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ed9SJMZuJrPf1pxLQ6n9lxYuTkZWqzz-\" width=45%></a>\n",
    "\n",
    "<a href=\"https://i.stack.imgur.com/OI6Gy.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1FgSbb7IY_54ew6YmWKd4ewXDKm-inM20\" width=45%></a>\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1tItTVqiI-p0PDYDLFZszKwnUOOxq6_Ba\"><img src=\"https://drive.google.com/uc?export=view&id=1paVigRCEZL2LlgLKUWXdyl2yKC9MGTEz\" width=45%></a>\n",
    "\n",
    "__The same procedure can be applied in arbitrary many dimensions.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Descent method\n",
    "\n",
    "The above discussion means in practice, that:\n",
    "\n",
    "in a problemspace with however many parameters, __the gradient vector gives an update direction for every parameter at once towards a nearby minima__ based on a linear approximation.\n",
    "\n",
    "So the main method is as follows:\n",
    "- Start from randomly initialized parameters\n",
    "- Take the derivative of the function to be optimized with respect to the current parameters, this gives the gradient vector\n",
    "- Take a step (modify the parameters) with $-1*\\lambda*gradients$ (Where $\\lambda$ is a \"learning rate\", ensuring that we do not take a step too far away from the currrent point, where the approximation is miserable. It is usually in the order of magnitude of eg. $0.001$ or so.)\n",
    "- Repeat, until gradient is \"zero\"\n",
    "\n",
    "<a href=\"http://hduongtrong.github.io/assets/gradient_descent/gradient_descent.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1A1Qa3dwJoItJ2TY5ZjlddRpDUKH204e4\" width=45%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties and limitations\n",
    "\n",
    "### Speed\n",
    "\n",
    "Why did we put double quote marks around \"zero\" above, or, to phrase it differently: do we see any problems with convergence in case of this last animation above?\n",
    "\n",
    "The main observation here is, that as we move closer and closer to the minima, the magnitude of gradients gets successively smaller and smaller. This is somewhat beneficial for preventing the \"overstepping\" of the minima, but makes convergence exactly zero extremely painful, so in practice, some __$\\epsilon$ tolerance is used as stopping criteria__ instead of zero. \n",
    "\n",
    "None the less, one can see, that there is drastic slowdown visible in convergence behavior, in fact, one can show, that __majority of the time the algorithm spent on realizing diminishing returns__. This makes \"stopping\" a problematic endeavor, and many times results in either excessive amounts of computation or suboptimal results.\n",
    "\n",
    "The motivation for applying \"higher order\" methods, relying on the already mentioned second derivatives comes exactly from this effect, but leads to it's own problems (computational intractability or diverging behavior in case of \"far from minima\" starting points). In practice, when one is lucky (rarely) one can and should start with gradient descent, and after some steps switch over to high order methods.\n",
    "\n",
    "### Local minima\n",
    "\n",
    "The other, major problem with GD - as one might have already guessed - is, that it relies strictly on local information. The first derivative tells us about inflection points, the sceond derivative could inform us about local curvature, but none of these tells us about global optima - unless one can solve for the second derivative analytically.\n",
    "\n",
    "<a href=\"https://lucidar.me/en/neural-networks/files/gradient-local-minima.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KCLRDkclC2yG8emAyby76aMGX4PstbIF\" width=35%></a>\n",
    "\n",
    "Also worth mentioning, that the gradient can be zero in case of a more \"frustrating\" topology, namely in \"saddle points\", where one parameter has a local minimum, and another(s) a maximum. Please remember, that the first derivative can not distinguish minima from maxima, so the gradient will be zero. (Or converging towards it painfully slowly, for that matter...)\n",
    "\n",
    "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Saddle_point.svg/1200px-Saddle_point.svg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1yFNLgvWeR3e4T6vWwR6j4tBlF1SE3T35\" width=\"45%\"></a>\n",
    "\n",
    "So in practice, convergence will only happen towards the closest local minima, where the procedure gets \"stuck\". For some approach to mitigating this effect, see our separate notebook on GD variants.\n",
    "\n",
    "\n",
    "Bearing all these limitations in mind, none the less, gradient descent is still the \"workhorse\" of function optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good alternative presentation of the whole topic of Gradient Descent can be found [here](https://www2.seas.gwu.edu/~simhaweb/contalg/modules/module10/module10.html). TODO maybe talk about bracket search above??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
