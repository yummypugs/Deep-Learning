{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load data into a model?\n",
    "\n",
    "Since in the most wide spread (and presumably most stably generalizing) case of minibatch gradient descent, we are presupposing the ability to iterate through (with some kind of generator) the data multiple times (epoch).\n",
    "\n",
    "We have the choice to build up our own generators, use external tools (as for example [Blaze](http://blaze.pydata.org/) / [Dask](http://docs.dask.org/en/latest/why.html)), or utilize TensorFlow's [Data API](https://www.tensorflow.org/guide/datasets). This question is crucial for performance, since if we utilize GPUs, they have separate, dedicated memory only accessible via a copy operation from the main RAM, which even utilizes the CPU as well as the internal \"bus\" interfaces, thus it can become the single biggest bottleneck for training. \n",
    "\n",
    "Plain English version: Even if you by an expensive GPU, if you load data inefficiently, training will be super slow.\n",
    "\n",
    "For some more guidance on the design of high performance models see the [guide](https://www.tensorflow.org/guide/data_performance) by the TF team.\n",
    "\n",
    "## The tf.data API\n",
    "\n",
    "For the data API the documentation is surprisingly informative:\n",
    "\n",
    "\"The tf.data API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\n",
    "\n",
    "The tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n",
    "\n",
    "There are two distinct ways to create a dataset:\n",
    "\n",
    "A data source constructs a Dataset from data stored in memory or in one or more files.\n",
    "\n",
    "A data transformation constructs a dataset from one or more tf.data.Dataset objects.\"\n",
    "\n",
    "The whole approach bears some resemblance to Scikit's pipeline, albeit with more emphasis on data cleaning and manipulation, and less on the successive models - since typically we use TF for neural models, that contain the feature extractor hierarchy inside them.\n",
    "\n",
    "Also, there is a very elaborate mechanism for parallel load from the filesystem in a streaming manner, as well as efficient usage of the TFRecord format included.\n",
    "\n",
    "## Dataset usage example\n",
    "\n",
    "Dataset has a functional style usage whereby we can chain together preparation steps for our data. \n",
    "\n",
    "**Definition:**\n",
    "\n",
    "```python\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))\n",
    "```\n",
    "\n",
    "**Alternatively:**\n",
    "\n",
    "```python\n",
    "np_array = np.random.randint(low=-10, high=10, size=10)\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(np_array)\n",
    "```\n",
    "\n",
    "\n",
    "**Iteration:**\n",
    "\n",
    "The thus resulting dataset is an \"iterable\", thus can handle the following\n",
    "\n",
    "\n",
    "```python\n",
    "for i in training_dataset:\n",
    "    print(i)\n",
    "```\n",
    "\n",
    "\n",
    "## Further enhancements\n",
    "\n",
    "By default it is advised to use `Dataset` for feeding your data, and even utilizing `Dataset.prefetch()`, to gain some speed (see discussion [here](https://stackoverflow.com/questions/47064693/tensorflow-data-api-prefetch)), as well as using `TFRecord` file format (see this [post](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564)).\n",
    "\n",
    "The complete performance guide can be found [here](https://www.tensorflow.org/performance/datasets_performance).\n",
    "\n",
    "\n",
    "## Getting things to work with Keras\n",
    "\n",
    "### Fits in memory\n",
    "\n",
    "The default case is, when the whole data fits nicely into memory.\n",
    "\n",
    "The design pattern (in case of _sequential API_) is:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load entire dataset\n",
    "X, y = np.load('some_training_set_with_labels.npy')\n",
    "\n",
    "# Design model\n",
    "model = Sequential()\n",
    "model.add(Dense(..., input_shape=(784,))) \n",
    "# WARNING!\n",
    "# THIS is equivalent!!!\n",
    "# model.add(Dense(..., input_dim=784))\n",
    "\n",
    "[...] # Your architecture\n",
    "model.compile()\n",
    "\n",
    "# Train model on your dataset\n",
    "model.fit(X,y,...)\n",
    "```\n",
    "\n",
    "### The `generator` way\n",
    "\n",
    "In the not so trivial case, when you don't fit into memory, you either use a default generator (available for images, text and sequences in Keras), or you write your own generator function\n",
    "\n",
    "Keras is not detailing this too much, see [here](https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory), but then gives a specific example [here](https://keras.io/utils/#sequence) (There is a bit more annotated version you can find [here](https://medium.com/datadriveninvestor/keras-training-on-large-datasets-3e9d9dbc09d4)). \n",
    "\n",
    " ```python\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class CIFAR10Sequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            resize(imread(file_name), (200, 200))\n",
    "               for file_name in batch_x]), np.array(batch_y)\n",
    "```\n",
    "\n",
    "```python\n",
    "my_generator = CIFAR10Sequence(X,y,batch_size)\n",
    "\n",
    "model.fit(my_generator,...)\n",
    "```\n",
    " \n",
    "A good description of writing own generator functions can be found [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly).\n",
    "\n",
    "\n",
    "### Fitting directly on a `Dataset`\n",
    "\n",
    "But if we already made ourselves familiar with the TF Dataset API, why not use just that?\n",
    "\n",
    "```python\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "my_dataset = Dataset.from_tensor_slices(x).repeat().batch(...)\n",
    "\n",
    "...\n",
    "\n",
    "model.fit(my_dataset,steps_per_epoch=1, epochs=...)\n",
    "```\n",
    "\n",
    "Here we see, that: \"Starting from Tensorflow 1.9, one can pass tf.data.Dataset object directly into keras.Model.fit() and it would act similar to fit_generator.\"\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/46135499/how-to-properly-combine-tensorflows-dataset-api-and-keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
