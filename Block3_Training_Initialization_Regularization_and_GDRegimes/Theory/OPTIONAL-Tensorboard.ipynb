{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPeDoT144gEj"
   },
   "source": [
    "# How to \"see\" what's going on during training?\n",
    "\n",
    "Since during training - especially for early stopping - we would like to have \n",
    "\n",
    "- Model graph visualization\n",
    "- Visualization measured scalars \n",
    "    - Training metrics (train and \n",
    "    - Weights based metrics (histogram of weights)\n",
    "\n",
    "So instead of simple printouts, we can get interactive charts, like this:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/mnist_tensorboard.png\" width=600 heigth=600>\n",
    "\n",
    "Generally TensorBoard's structure rests on the predicament of being able to access a continuously growing **log** of events which occur during training.\n",
    "\n",
    "\n",
    "## Logging in TF: `tf.summary`\n",
    "\n",
    "With the help of [tf.summary](https://www.tensorflow.org/api_docs/python/tf/summary) tools we are able to collect measurement during the `Session`, and save them to a pre-defined folder.\n",
    "\n",
    "### How to save summaries in \"barebones\" TF?\n",
    "\n",
    "```python\n",
    "\n",
    "def train(model, optimizer, dataset, log_freq=10):\n",
    "  avg_loss = tf.keras.metrics.Mean(name='loss', dtype=tf.float32)\n",
    "  for images, labels in dataset:\n",
    "    loss = train_step(model, optimizer, images, labels)\n",
    "    avg_loss.update_state(loss)\n",
    "    if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "      tf.summary.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
    "      avg_loss.reset_states()\n",
    "\n",
    "def test(model, test_x, test_y, step_num):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  loss = loss_fn(model(test_x, training=False), test_y)\n",
    "  tf.summary.scalar('loss', loss, step=step_num)\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer('/tmp/summaries/train')\n",
    "test_summary_writer = tf.summary.create_file_writer('/tmp/summaries/test')\n",
    "\n",
    "with train_summary_writer.as_default():\n",
    "  train(model, optimizer, dataset)\n",
    "\n",
    "with test_summary_writer.as_default():\n",
    "  test(model, test_x, test_y, optimizer.iterations)\n",
    "```\n",
    "In the now default \"eager\" execution mode, the ops are executed and thus the data is saved on the fly. (In TF1.0 \"graph mode\" one has to do a fetch and save operation explicitly.)  \n",
    "\n",
    "\n",
    "**To sum it up:**\n",
    "- We initialize a `tf.summary.FileWriter` object\n",
    "    - We use `./Graph` in our example, but can be any folder name.\n",
    "    - We give in as parameter our default graph in this case, just to show, that we would like to save metrics from this model. (Multiple models in multiple scopes can exist, with their own loggers...)\n",
    "- We write an element (in this case every update) to the summary\n",
    "- Logs will be written to the appropriate folder\n",
    "\n",
    "\n",
    "\n",
    "## Using TensorBoard over the log\n",
    "\n",
    "So if we have a constantly updating log, we can start to use TensorBoard.\n",
    "\n",
    "TensorBoard is a **different process** with a built in **webserver**, that we have to start separately, and by default it is listening on \n",
    "\n",
    "`http://localhost:6006/`.\n",
    "\n",
    "To start TensorBoard, use the command \n",
    "\n",
    "`TensorBoard --logdir ...`\n",
    "\n",
    "After this, navigate to the webaddress above where you can interact with the log data.\n",
    "\n",
    "Short tutorial on TensorBoard can be found [here](https://medium.com/@anthony_sarkis/tensortoard-quick-start-in-5-minutes-e3ec69f673af), the documentation [here](https://www.tensorflow.org/guide/summaries_and_TensorBoard)\n",
    "\n",
    "The approach of TensorBoard is scalable, is a separate logging collector / viewer machine is used, it can scale to training clusters. (See [here](https://learnk8s.io/infiniteconf2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz5uSrqA4gEk"
   },
   "source": [
    "## How to use TesorBoard with Keras?\n",
    "\n",
    "Keras is having a concept of `callbacks`, which are executed at various points in training, like at the beginning / end of a minibatch or the beginning / end of an epoch.\n",
    "\n",
    "Keras comes with a built in facility for connecting in a TensorBoard logger as a callback pretty easily.\n",
    "\n",
    "The design pattern is as follows:\n",
    "\n",
    "```python\n",
    "from keras.callbacks import TensorBoard\n",
    "...\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', update_freq='epoch')\n",
    "...\n",
    "\n",
    "model.fit(x_train, y_train_cat,\n",
    "         epochs=..,\n",
    "         validation_data=(x_test, y_test_cat),\n",
    "         callbacks=[tensorboard]) \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The saved logs will be processed by TensoBoard normally. It is worth noting, that there are many customization possibilites, eg. to let the callback save gradient histograms, weight histograms etc.\n",
    "\n",
    "Same general design patter is true for `ModelCheckpoint`-s, which save our models, which are super handy, especially with the `save_best_only=True` parameter enabled.\n",
    "\n",
    "## Getting TensorBoard to work in Colab\n",
    "\n",
    "Since Colab is a remote environment, running a new webserver is not totally straightforward.\n",
    "\n",
    "Luckily, Google worked on the nice integration of Tensorboard to Colab.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Load the TensorBoard extension\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "```\n",
    "\n",
    "### Add to tf.keras callback\n",
    "\n",
    "Same as before...\n",
    "\n",
    "```python\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "```\n",
    "\n",
    "### Start TensorBoard within the notebook using magics function\n",
    "\n",
    "```python\n",
    "%tensorboard --logdir logs/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DEPRECATED_Day7-Practice_How_to_load_data_TF_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
